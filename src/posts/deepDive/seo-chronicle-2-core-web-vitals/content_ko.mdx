---
title: 'SEO의 과거, 현재, 그리고 미래 | 성능의 시대: 기술적 SEO와 사용자 경험 #2'
desc: 전쟁은 끝났지만 전장은 모바일로 이동했습니다. 모바일 게돈부터 Core Web Vitals까지. 키워드를 넘어 기술적 완성도와 사용자 경험(UX)이 중요해진 '테크니컬 SEO'의 시대를 프론트엔드 개발자의 시선으로 정리합니다.
date: 2025-12-01
thumbnail: /posts/deepDive/seo-chronicle-2-core-web-vitals/thumbnail.avif
---

## 플랫폼의 변화, 그리고 새로운 기준

1부에서 다룬 '구글과 블랙햇의 공방'이 검색 결과의 신뢰성을 회복하는 과정이었다면 2010년대 중반부터는 검색 환경 자체의 패러다임이 바뀌기 시작했습니다. 스마트폰이 보급되면서 사용자의 주 활동 무대가 PC에서 **모바일**로 급격하게 이동했기 때문입니다.

이제 구글은 단순히 콘텐츠의 질을 넘어 "얼마나 쾌적한 모바일 경험을 제공하는가"를 핵심 평가 기준으로 삼기 시작했습니다. 또한 머신러닝 기술의 도입으로 검색 엔진은 단순한 키워드 매칭을 넘어 인간의 언어와 의도를 맥락적으로 이해하게 되었습니다.

이러한 변화 속에서 SEO의 핵심은 마케팅적 기법에서 기술적 완성도로 확장되었습니다. 이번 2부에서는 모바일과 AI가 불러온 검색 환경의 변화, 그리고 이에 대응하기 위해 프론트엔드 개발자가 알아야 할 테크니컬 SEO(Technical SEO)의 핵심 전략들을 다뤄보겠습니다.

---

## 챕터 1. 시대의 변화: 키워드에서 경험으로

본격적인 기술 대응에 앞서 구글이 지난 10년간 개발자들에게 어떤 시그널을 보냈는지 이해할 필요가 있습니다. 구글의 메시지는 일관되었습니다. "사용자를 불편하게 만들지 마라."

### 1. 모바일 게돈(Mobilegeddon)과 반응형 웹의 부상
![2015/04/21 Rolling out the mobile-friendly update](/posts/deepDive/seo-chronicle-2-core-web-vitals/mobilegeddon.avif)

2015년 4월, 구글은 "모바일 친화적이지 않은 사이트는 검색 순위를 내리겠다"고 선언합니다. 업계에서는 이를 성경의 아마겟돈에 빗대어 '모바일 게돈(Mobilegeddon)'이라 불렀습니다.

이때 개발자들은 중요한 기로에 섰습니다. 모바일에 대응하는 방식은 크게 두 가지였기 때문입니다.
* **별도 URL (Separate URLs):** `m.youtube.com`처럼 모바일 전용 사이트를 따로 만드는 방식입니다. 모바일에 최적화된 가벼운 소스를 제공할 수 있다는 장점이 있어 대형 서비스들은 여전히 사용하지만 PC와 모바일 페이지 간의 관계(`rel="canonical"`과 `rel="alternate"`)를 정확히 명시해야 하는 SEO적 난이도가 있었고 유지보수 비용이 두 배로 드는 치명적인 단점이 있었습니다.
<img src="/posts/deepDive/seo-chronicle-2-core-web-vitals/separate.avif" alt="m.youtube.com image" width='300' style={{margin: 'auto'}} />

* **반응형 웹 (Responsive Web Design):** 하나의 URL에서 화면 크기에 따라 레이아웃이 유동적으로 변하는 방식입니다. 단일 URL을 사용하므로 크롤링 효율이 높고 링크의 권한(Link Equity)이 분산되지 않습니다.

결국 구글은 SEO 효율성을 이유로 **반응형 웹**을 공식 권장했고 이는 프론트엔드 개발자가 단일 코드베이스로 멀티 디바이스를 대응하는 기술을 필수로 익혀야 하는 결정적인 계기가 되었습니다.

### 2. 랭크브레인(RankBrain): 의도를 읽는 AI
2015년 10월, 구글은 머신러닝 기반의 알고리즘인 **랭크브레인(RankBrain)** 의 도입을 [공식 발표했습니다.](https://searchengineland.com/meet-rankbrain-google-search-results-234386) 이는 단순한 키워드 매칭의 시대를 끝내는 신호탄이었습니다.

>  For example, if you search for “what’s the title of the consumer at the highest level of a food chain,” our systems learn from seeing those words on various pages that the concept of a food chain may have to do with animals, and not human consumers. By understanding and matching these words to their related concepts, RankBrain understands that you’re looking for what’s commonly referred to as an “apex predator.”
출처: [RankBrain — a smarter ranking system](https://blog.google/products/search/how-ai-powers-great-search-results/)

구글은 이를 단어(Words)와 개념(Concepts)을 연결하는 기술이라고 설명합니다. 예를 들어 "사용자가 먹이사슬 최상위 단계에 있는 소비자를 뭐라고 부르지?" 라고 길게 풀어서 검색하더라도 랭크브레인은 이 문맥이 구매자(Consumer)가 아닌 동물 생태계 이야기임을 파악하고 최상위 포식자(Apex Predator)라는 정확한 개념과 연결해 줍니다.

이 변화는 개발자에게 새로운 과제를 던졌습니다. 이것이 구글 봇이 웹사이트의 콘텐츠가 무엇을 의미하는지 명확히 파악할 수 있도록 돕는 **시맨틱 구조(Semantic Structure)** 와 **구조화된 데이터(Structured Data)** 가 필수적인 요소로 자리 잡게 된 배경입니다.

### 3. Core Web Vitals: 사용자 경험의 정량화
그리고 2020년, 구글은 **Core Web Vitals**를 공식 랭킹 요소로 반영하겠다고 발표합니다.

이는 단순히 "사이트가 빠르면 좋다"는 권고를 넘어 로딩 속도(LCP), 반응성(FID/INP), 시각적 안정성(CLS)을 구체적인 수치로 측정하고 이를 검색 순위에 반영하겠다는 결정이었습니다. 즉, 막연했던 '사용자 경험'이 측정 가능한 '기술적 지표'로 바뀌게 된 것입니다.
이로 인해 SEO는 단순히 검색 상위 노출을 노리는 것을 넘어, **SXO(Search Experience Optimization, 검색 경험 최적화)** 라는 더 넓은 개념으로 확장되었습니다.

### 4. 시장의 변화: 마케팅에서 엔지니어링으로
알고리즘의 고도화는 SEO 시장의 풍경도 바꾸어 놓았습니다. 과거에는 단순히 키워드 작업이나 백링크를 통해 단기간에 상위 노출을 보장한다는 업체들이 많았지만 판다와 펭귄 업데이트 이후 이러한 방식은 효율성을 잃게 되었습니다.

이제는 사이트의 기술적 결함(로딩 속도, 크롤링 오류, 보안 등)을 진단하고 구조적인 해결책을 제시하는 테크니컬 SEO가 필수적인 요소가 되었습니다. 기업들 역시 SEO를 단순한 마케팅 활동이 아닌 개발팀이 주도해야 할 엔지니어링 과제로 인식하게 된 것입니다.

---

## 챕터 2. [Performance] 0.1초의 승부: Core Web Vitals 대응 전략

Core Web Vitals의 도입으로 SEO는 이제 프론트엔드 개발자의 핵심 성과 지표(KPI)가 되었습니다. 하지만 눈대중으로 "빨라진 것 같다"고 판단할 수는 없습니다. 구글은 개발자들이 이를 객관적으로 측정할 수 있도록 **Lighthouse**라는 도구를 제공했습니다.

![lighthouse](/posts/deepDive/seo-chronicle-2-core-web-vitals/lighthouse-report_1920.avif)

Chrome DevTools에 내장된 Lighthouse는 웹페이지의 성능, 접근성, SEO 등을 0점에서 100점까지 수치화하여 보여줍니다. 개발자에게 Lighthouse의 퍼포먼스 점수는 단순한 숫자가 아니라 현재의 기술적 최적화 수준을 객관적인 데이터로 검증하고 판단하는 중요한 기준이 됩니다.

Lighthouse 점수를 90점 이상(초록색)으로 끌어올리기 위해서는 구글이 제시한 다음 3가지 핵심 지표를 집중적으로 공략해야 합니다.

![core web vitals](/posts/deepDive/seo-chronicle-2-core-web-vitals/vitals.avif)

### 1. LCP (Largest Contentful Paint): 로딩 속도
LCP는 사용자가 페이지에 진입했을 때 **가장 큰 콘텐츠(보통 메인 히어로 이미지나 텍스트)** 가 렌더링 되기까지의 시간을 측정합니다. 구글은 이를 **2.5초 이내**로 유지할 것을 권장합니다.

* **이미지 최적화:** 웹페이지 용량의 대부분을 차지하는 것은 이미지입니다. 단순히 용량을 줄이는 것을 넘어 시스템적인 최적화가 필요합니다.
    * **차세대 포맷:** 무거운 JPG/PNG 대신 압축 효율이 월등한 `webp`나 `avif` 포맷을 브라우저 지원 여부에 따라 분기 처리하여 제공합니다.
    * **우선순위 힌트:** 브라우저가 중요한 이미지를 먼저 다운로드하도록 `<img fetchpriority="high">` 속성을 사용하거나, Next.js의 `next/image` 컴포넌트를 활용해 LCP 요소의 로딩 순위를 앞당깁니다.
* **CDN 리사이징 전략:** 원본 이미지를 그대로 전송하는 것은 엄청난 리소스 낭비입니다. Akamai나 CloudFront 같은 이미지 CDN을 활용해 클라이언트가 `image.jpg?w=640`과 같이 요청하면 서버리스 함수가 실시간으로 해당 디바이스 폭에 딱 맞는 이미지를 생성해 내려주도록 파이프라인을 구축해야 합니다.
* **서버 응답 시간(TTFB) 단축:** 아무리 프론트엔드를 최적화해도 HTML 문서 자체가 늦게 도착하면 소용없습니다. 동적인 데이터가 필요한 부분과 정적인 부분을 구분해서 공통된 레이아웃과 콘텐츠는 CDN에 캐싱하여 즉시 내려주고 개인화된 정보는 클라이언트 사이드에서 비동기(Fetch)로 불러오거나 스트리밍 방식으로 채워 넣는 전략을 통해 초기 응답 속도를 획기적으로 줄여야 합니다.

### 2. CLS (Cumulative Layout Shift): 시각적 안정성
기사를 읽고 있는데 갑자기 상단 광고가 늦게 로딩되면서 텍스트가 툭 밀려나 엉뚱한 곳을 클릭하게 된 경험, 다들 있으실 겁니다. CLS는 이런 예기치 않은 레이아웃 이동을 측정합니다. 0.1 이하를 유지해야 합니다.

* **공간 미리 확보(Aspect Ratio):** 이미지가 로딩되기 전이라도 브라우저가 해당 공간을 비워두도록 해야 합니다. CSS의 `aspect-ratio` 속성을 사용하거나, 이미지 태그에 명시적으로 `width`와 `height` 값을 넣어 레이아웃이 무너지는 것을 방지합니다.
* **폰트 로딩 전략:** 웹 폰트가 로딩되기 전과 후의 글자 크기 차이로 인해 줄바꿈이 달라지며 레이아웃이 흔들릴 수 있습니다. `font-display: swap`이나 `optional` 옵션을 사용하고, CSS의 `size-adjust` 속성을 통해 시스템 폰트와 웹 폰트의 크기 차이를 미세하게 보정하여 시각적 안정성을 확보합니다.

### 3. INP (Interaction to Next Paint): 상호작용
과거의 **FID(First Input Delay)** 가 사용자의 '첫 클릭'이 얼마나 빨리 반응하는지만 측정했다면 2024년 3월부터 정식 도입된 INP는 페이지에 머무는 동안 발생하는 모든 상호작용(클릭, 탭, 키보드 입력)의 응답 속도를 측정합니다. 훨씬 더 까다로운 지표입니다.

* **메인 스레드(Main Thread) 차단 방지:** 무거운 자바스크립트 연산이 메인 스레드를 점유하고 있으면 사용자가 버튼을 눌러도 브라우저는 반응할 수 없습니다. Chrome DevTools의 Performance 탭을 분석하여 실행 시간이 긴 작업(Long Tasks)을 잘게 쪼개거나 급하지 않은 로직은 `requestIdleCallback` 등을 활용해 유휴 시간으로 미루는 최적화가 필요합니다.
* **하이드레이션(Hydration) 최적화:** React 기반 앱의 경우, 초기 HTML이 로드된 후 자바스크립트 이벤트가 연결되는 하이드레이션 과정에서 반응성이 떨어질 수 있습니다. 화면에 보이지 않는 컴포넌트의 로딩을 지연(Lazy Loading)시키거나 상호작용이 필요한 부분만 선택적으로 하이드레이션하는 아일랜드 아키텍처(Island Architecture)의 개념이 도입되고 있습니다.

### 4. FCP (First Contentful Paint)
Core Web Vitals 3대장에는 포함되지 않지만 프론트엔드 개발자라면 **FCP(First Contentful Paint)** 를 절대 간과해서는 안 됩니다. 사용자가 '아, 접속됐구나'라고 느끼는 첫 순간이기 때문입니다.

사용자가 화면에서 '무언가'를 처음 보는 시점입니다. 텅 빈 흰 화면(White Screen)을 보는 시간을 줄여야 이탈률을 낮출 수 있습니다. 렌더링을 차단하는 리소스(Render-Blocking Resources)를 제거하는 것이 핵심입니다. 중요한 CSS를 인라인으로 처리하거나 페이지 로딩에 필수적이지 않은 서드파티 스크립트(광고, 분석 툴 등)는 `defer`나 `async` 속성을 사용해 로딩 순서를 뒤로 미루는 최적화가 필요합니다.
<small>물론 정말 급할 땐 setTimeout(..., 0)으로 슬쩍 미루는 '고전적인 비기'도 있긴 합니다🧙🏼‍♂️</small>

---

## 챕터 3. [Architecture] 프레임워크와 아키텍처: Next.js를 활용한 기술적 대응

성능 최적화(챕터 2)가 사용자의 쾌적한 경험을 위한 것이라면 렌더링 전략과 메타데이터 관리는 우리 사이트를 분석하러 온 검색 봇(Bot)을 위한 배려입니다.
과거에는 이 두 가지를 모두 잡기 위해 많은 공수가 필요했지만 모던 웹 프레임워크인 Next.js를 활용하면 봇 친화적인 아키텍처를 효율적으로 구축할 수 있습니다.

### 1. 렌더링 전략의 다변화 (SSR/SSG)
과거의 SPA(Single Page Application)에서 주로 사용하던 CSR(Client Side Rendering) 방식은 초기 로딩 시 빈 화면만 보여주기 때문에 검색 봇이 콘텐츠를 수집하는 데 치명적인 한계가 있었습니다.
이 문제를 해결하기 위해 **Next.js** 와 같은 프레임워크를 도입하여 페이지의 목적에 맞는 렌더링 전략을 수립하는 것이 일반적입니다.

* **SSR (Server Side Rendering):** 검색 결과 페이지나 마이페이지 등 실시간 데이터가 중요한 페이지는 요청 시 서버에서 HTML을 완성해 내려주어 봇이 즉시 유의미한 텍스트를 읽을 수 있게 합니다.
* **SSG (Static Site Generation):** 회사 소개, 약관 등 변경이 적은 페이지는 빌드 타임에 미리 HTML을 생성해 두어 최고의 로딩 속도를 확보합니다.
* **ISR (Incremental Static Regeneration):** 블로그나 상품 상세처럼 적당한 업데이트가 필요한 경우 정적 페이지의 빠른 속도를 유지하면서도 설정한 주기마다 백그라운드에서 최신 데이터를 반영하는 ISR이 매우 효과적입니다.

### 2. 내장 컴포넌트를 통한 성능 자동화
LCP와 CLS 같은 Core Web Vitals 지표를 일일이 코드로 잡는 것은 매우 번거로운 작업입니다. Next.js는 이를 위한 전용 컴포넌트를 제공하여 최적화를 자동화합니다.

* **이미지 최적화 (`next/image`):** 기존 `<img>` 태그 대신 사용합니다. 브라우저 환경에 맞춰 webp/avif 포맷으로 자동 변환하고 디바이스 크기에 맞게 리사이징하여 LCP 성능을 극대화합니다. 또한 `width/height`를 기반으로 공간을 미리 확보하여 CLS를 방지합니다.
* **폰트 최적화 (`next/font`):** 구글 폰트 등을 빌드 타임에 다운로드하여 호스팅합니다. 폰트 로딩 시 텍스트가 깜빡이거나 흔들리는 현상(FOUT/FOIT)을 `size-adjust` 속성 자동 조절로 해결하여 시각적 안정성을 확보합니다.

### 3. 메타데이터의 프로그래머틱 관리
수천 개의 페이지에 각각 다른 메타 태그(`title`, `description`, `og:image`)를 달아주는 것은 수동으로 관리하기 어렵습니다.
Next.js의 Metadata API를 활용하면 페이지의 데이터를 기반으로 SEO 태그를 동적으로 생성할 수 있습니다.

```javascript
// app/hotels/[id]/page.js 예시
export async function generateMetadata({ params }) {
  const hotel = await getHotelInfo(params.id);

  return {
    title: `${hotel.name} - 오사카 추천 호텔`,
    description: hotel.summary,
    openGraph: {
      images: [hotel.thumbnailUrl],
    },
  }
}
```

### 2. 크롤링 예산(Crawl Budget)과 인덱싱
마지막으로, 봇이 사이트를 효율적으로 탐색하도록 길을 터주는 작업입니다. 서비스 규모가 커질수록 봇이 하루에 긁어갈 수 있는 페이지의 수, 즉 크롤링 예산이 부족해지는 문제가 발생합니다. 봇이 중요한 페이지를 놓치지 않고 효율적으로 탐색하도록 유도해야 합니다.

* **HTML Sitemap 구축:** `sitemap.xml` 파일은 기본입니다. 여기에 더해 봇이 링크를 타고 사이트 깊숙한 곳(Deep Link)까지 도달할 수 있도록 서비스 내 모든 중요 카테고리와 링크를 모아둔 HTML Sitemap 페이지를 별도로 구축합니다. 이는 사용자에게는 사이트 맵 역할을 봇에게는 훌륭한 내비게이션 역할을 합니다.
<img src="/posts/deepDive/seo-chronicle-2-core-web-vitals/onsen.avif" alt="skyticket onsen" width='300' style={{margin: 'auto'}} />
<small>2023년 당시 제가 seo팀에서 일 하던 때 만들었던 HTML Sitemap 페이지 입니다. 하위 페이지로 연결되는 링크를 모아두어 검색 봇이 구석구석 크롤링할 수 있도록 유도했습니다.</small>

* **URL 정규화 (Canonical & Trailing Slash):** `example.com/page`와 `example.com/page/`는 기술적으로 다른 URL입니다. 검색 엔진이 이를 중복 콘텐츠로 오해하여 랭킹을 분산시키는 것을 막기 위해 **Trailing Slash** 정책을 통일하고 모든 페이지에 **Canonical 태그**를 적용해 원본 URL을 명확히 지정해야 합니다.

---

## 챕터 4. [Semantic] 의미의 전달: 기계가 이해하는 언어

마지막으로 콘텐츠의 구조와 의미를 기계에게 명확히 전달하는 작업입니다. 이는 검색 결과에서 우리 사이트가 더 매력적으로 보이게 만드는 핵심 전략입니다.

### 1. 시맨틱 마크업 (Semantic Markup)
디자인을 위해 `div`와 `span`을 남발하던 습관을 버리고 의미론적인 태그를 적극적으로 사용해야 합니다.
* 헤더는 `header`, 내비게이션은 `nav`, 본문은 `article`, 사이드바는 `aside`, 푸터는 `footer`를 사용하여 문서의 구획을 명확히 합니다.
* 특히 `h1`부터 `h6`까지 제목 태그의 위계를 논리적인 목차 순서대로 정리하여 기계가 문서의 핵심 주제와 하위 내용을 쉽게 파악할 수 있도록 해야 합니다.
<img src="/posts/deepDive/seo-chronicle-2-core-web-vitals/semantic.avif" alt="ikyu.com Semantic Markup" width='300' style={{margin: 'auto'}} />
<small>HeadingsMap와 같은 확장 프로그램으로 h태그의 구조를 쉽게 파악할 수 있습니다.</small>

### 2. 구조화된 데이터 (JSON-LD)와 리치 스니펫
검색 결과에 단순히 파란색 링크와 설명만 나오는 것이 아니라 별점, 가격, 리뷰 수, FAQ 등이 노출되는 것을 보셨을 겁니다. 이를 **리치 스니펫(Rich Snippets)** 이라고 합니다.
`JSON-LD` 형식을 사용하여 페이지에 구조화된 데이터를 심는 것은 필수적입니다. "이 숫자는 가격이고, 이 텍스트는 리뷰입니다"라고 구글 봇에게 직접적으로 알려주는 것입니다. 이는 검색 결과에서 시선을 사로잡아 클릭률(CTR)을 높이는 데 결정적인 역할을 합니다.
![https://smartranking.nl/en/rich-snippets/](/posts/deepDive/seo-chronicle-2-core-web-vitals/rich-snippets-seo.webp)

### 3. 공유 경험 최적화 (OG 태그)
검색 엔진은 아니지만 트래픽 유입의 큰 축인 SNS 공유를 위해 **OG(Open Graph) 태그**를 페이지별로 동적으로 생성합니다. SNS나 슬랙에 링크를 올렸을 때 페이지의 내용을 요약한 매력적인 썸네일과 제목이 나오도록 하여 잠재 사용자의 유입을 유도하는 방법입니다.

![https://whitep4nth3r.com/blog/level-up-your-link-previews-in-slack/](/posts/deepDive/seo-chronicle-2-core-web-vitals/slack_preview.avif)

---

## 챕터 5. [Infra & Data] 테크니컬 SEO의 완성: 측정과 가속

SEO는 코드 레벨의 최적화로 끝나지 않습니다. 개발자가 만든 사이트가 전 세계 사용자에게 얼마나 빠르게 전달되는지(가속), 그리고 사용자가 들어와서 어떻게 행동하는지(측정)를 관리하는 인프라 영역까지가 테크니컬 SEO의 완성입니다.

### 1. CDN (Content Delivery Network): 물리적 한계를 넘는 속도
도쿄에 있는 서버의 데이터를 뉴욕 사용자가 요청하면 물리적 거리로 인해 지연(Latency)이 발생할 수밖에 없습니다. 이러한 물리적 한계를 극복하기 위해 **CDN(Content Delivery Network)** 도입은 필수적입니다. **Cloudflare**나 **AWS CloudFront**, **Akamai** 등이 대표적인 서비스입니다.

CDN은 테크니컬 SEO의 핵심인 '속도'와 '보안'을 물리적인 네트워크 레벨에서 해결해 주는 강력한 인프라입니다.

* **엣지 캐싱(Edge Caching):** 전 세계 수백 개의 엣지 서버(Edge Server)에 사이트의 정적 리소스를 미리 캐싱해 둡니다. 사용자가 접속하면 가장 가까운 서버에서 데이터를 즉시 전송하므로 TTFB(서버 응답 시간)와 LCP를 획기적으로 단축시킬 수 있습니다.
* **이미지 자동 최적화:** 서버에서 이미지를 하나하나 가공하지 않아도 CDN 레벨에서 접속자의 브라우저 환경에 맞춰 `webp/avif` 변환이나 리사이징을 자동화할 수 있습니다. (예: Cloudflare Images, CloudFront Lambda@Edge)
* **보안과 신뢰성 (HTTPS):** 구글은 HTTPS를 공식 랭킹 요소로 반영합니다. CDN을 사용하면 복잡한 인증서 관리 없이도 손쉽게 HTTPS를 적용할 수 있으며 디도스(DDoS) 공격이나 악성 봇 트래픽을 1차적으로 걸러내어 검색 봇이 원본 서버 리소스를 온전히 사용할 수 있도록 돕습니다.

### 2. Google Analytics 4 (GA4) & GTM: 데이터 기반 검증
과거의 SEO가 '직감'이었다면 현대의 SEO는 '데이터'입니다. 수행한 성능 개선 작업이 실제로 비즈니스 성과로 이어졌는지 검증하는 과정이 반드시 필요합니다.

* **체류 시간과 이탈률:** Core Web Vitals를 개선한 뒤 실제 사용자의 체류 시간이 늘어났는가? 이를 검증하기 위해 GA4와 같은 분석 도구를 활용한 데이터 추적이 필수적입니다.
* **GTM을 통한 성능 방어:** 마케팅 팀에서 다양한 추적 코드를 심어달라고 요청할 때 이를 소스 코드에 직접 삽입하면 사이트 성능(LCP, TBT)이 저하될 수 있습니다. 개발자는 Google Tag Manager(GTM)를 도입하여 스크립트 로딩 시점을 제어하고 성능 저하를 방어해야 합니다.

---

## 챕터 6. 신뢰의 알고리즘: E-E-A-T와 개발자

속도와 구조가 완벽하더라도 담겨있는 콘텐츠가 '가짜'라면 검색 엔진은 그 사이트를 상위로 올리지 않습니다. 구글은 **E-E-A-T(경험, 전문성, 권위성, 신뢰성)** 라는 기준을 통해 콘텐츠의 품질을 평가합니다.
얼핏 보면 "글을 잘 쓰라"는 마케팅 영역의 이야기 같지만 여기서도 개발자가 기술적으로 지원해야 할 핵심적인 역할이 있습니다.

### 1. 저자 정보의 구조화 (Author Schema)
구글은 "누가 이 글을 썼는가?"를 중요하게 봅니다. 익명의 글보다 실명 전문가의 글을 더 신뢰하기 때문입니다.
블로그 포스트 하단에 단순히 텍스트로 이름을 적는 것을 넘어, **`Person` 스키마 마크업**을 적용하여 저자의 이름, 소속, SNS 링크 등의 정보를 구글 봇에게 명확히 전달하는 작업이 필요합니다. 이는 사이트의 **전문성(Expertise)** 과 **권위성(Authoritativeness)** 을 기계가 인식하는 데 큰 도움을 줍니다.

```json
{
  "@context": "https://schema.org",
  "@type": "Person",
  "name": "mintora",
  "jobTitle": "Frontend Engineer",
  "sameAs": ["https://github.com/mintora", "https://linkedin.com/in/mintora"]
}
```

### 2. 보안과 신뢰성 (HTTPS & Security)
E-E-A-T의 중심인 **신뢰성(Trustworthiness)** 은 사이트의 보안과 직결됩니다.
* **HTTPS 필수 적용:** 앞서 CDN 챕터에서 언급했듯 SSL 인증서가 없는 사이트는 브라우저에서 '주의 요함'으로 표시되며 검색 순위에서도 불이익을 받습니다.
* **운영 주체 명시:** 사이트 하단(Footer)에 운영자 정보, 연락처, 개인정보처리방침 등을 명확히 배치하고, 이를 `Organization` 스키마로 묶어 사이트의 투명성을 기술적으로 보증해야 합니다.

### 3. 실전 경험 콘텐츠
최근 추가된 **Experience(경험)** 지표는 "직접 겪어보고 썼는가?"를 묻습니다. 개발 블로그라면 단순히 이론을 나열하는 것보다 직접 작성한 코드 스니펫이나 트러블 슈팅 로그를 첨부하는 것이 점수에 유리합니다. 코드 블록의 가독성을 높이기 위해 Syntax Highlighting 라이브러리를 최적화하여 적용하는 것도 좋은 전략입니다.

> **💡 구글은 E-E-A-T를 어떻게 정의하는가**
> *"Google의 자동화 시스템은 우수한 콘텐츠의 순위를 매길 때 다양한 요소를 고려하도록 설계되었습니다. Google 시스템은 관련 콘텐츠를 식별한 후 가장 유용해 보이는 콘텐츠에 우선순위를 제공하려 합니다. 이를 위해 Google 시스템에서는 어떤 콘텐츠가 경험, 전문성, 권위, 신뢰성이라는 네 가지 덕목, 즉 E-E-A-T를 갖고 있는지 판단하기 위해 여러 가지 요소를 사용합니다."*
>
> *출처: [Google Search Central: Creating helpful, reliable, people-first content](https://developers.google.com/search/docs/fundamentals/creating-helpful-content)*

---

## 마치며: 개발자의 코드가 마케팅이 되는 순간

지금까지 1990년대의 초기 검색 엔진부터 현대의 테크니컬 SEO가 정립되기까지의 과정을 살펴보았습니다.
이 긴 흐름 속에서 확인할 수 있는 사실은 명확합니다. SEO의 중심축이 단순한 '검색 순위'에서 웹사이트가 제공하는 **사용자 경험(User Experience)** 과 기술적 완성도로 이동했다는 점입니다. 모바일 환경과 머신러닝의 도입은 SEO의 영역을 마케팅에서 엔지니어링으로 확장시키는 계기가 되었습니다.

SSR 도입, 이미지 최적화, 시맨틱 태그 작성. 프론트엔드 개발자에게는 당연한 '기본기'처럼 보이는 이 작업들이 이제는 실제 비즈니스의 성장을 견인하는 핵심 경쟁력이 되고 있습니다.
지금까지는 속도를 높이고 구조를 개선하며 검색 엔진과 사용자 모두를 만족시키기 위해 노력해 왔습니다. 하지만 만약 사용자가 더 이상 검색 결과를 클릭하지 않는다면 어떨까요?

사용자가 파란색 링크를 찾아다니는 대신 AI가 요약된 답변을 직접 제시하는 세상. 다음 3부에서는 생성형 AI시대와 GEO(Generative Engine Optimization), 그리고 변화하는 환경 속에서 프론트엔드 개발자가 준비해야 할 미래에 대해 알아보겠습니다.

---

#### 참고 문헌 및 추천 자료
* [Rolling out the mobile-friendly update (Google Search Central Blog, 2015)](https://developers.google.com/search/blog/2015/04/rolling-out-mobile-friendly-update)
* [Mobile-first indexing best practices (Google Search Central)](https://developers.google.com/search/docs/crawling-indexing/mobile/mobile-sites-mobile-first-indexing)
* [Meet RankBrain: The Artificial Intelligence That’s Now Processing Google Search Results](https://searchengineland.com/meet-rankbrain-google-search-results-234386)
* [How Google uses artificial intelligence In Google Search](https://searchengineland.com/how-google-uses-artificial-intelligence-in-google-search-379746)
* [Web Vitals (web.dev)](https://web.dev/articles/vitals)
* [Interaction to Next Paint (INP)](https://web.dev/articles/inp)
* [Image Optimization (Next.js Docs)](https://nextjs.org/docs/app/building-your-application/optimizing/images)
* [Understand how structured data works (Google Search Central)](https://developers.google.com/search/docs/appearance/structured-data/intro-structured-data)
* [Creating helpful, reliable, people-first content](https://developers.google.com/search/docs/fundamentals/creating-helpful-content)